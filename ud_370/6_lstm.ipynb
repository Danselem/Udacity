{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297518 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "o uoidi wghfhfq qce f upx  ctzitdzupirbxbwtvu aiaecyieti h fe onp i dsswnpytken \n",
      "rzkumol etr don wrm  o qkpe ejtxeve zmxubpgiewlonekwq sejhisafns hdet rvwjxaat s\n",
      "f fhhilqr fylnwcfgiqlnkekctnfvh ttjedxz asngvhz p dnthsfs  tdo  jowlle pliufhnom\n",
      "su hftwwedlfpexpimezfcelotiicf nl e cibf atie h tu s  qela pytve rz md i vd nhta\n",
      "owa ueegaoeemitnhyepdmrh t qqlig egvzxgvooaw zpvs pg crbv plnhelrtrgmcgf  hcqsvi\n",
      "================================================================================\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 100: 2.596052 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.98\n",
      "Validation set perplexity: 10.46\n",
      "Average loss at step 200: 2.250840 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 300: 2.105967 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 2.004456 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 500: 1.935769 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.910431 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 700: 1.860322 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 800: 1.820786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.832742 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.829602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "ques in the comres exreace handum in it is steevh colffre his ohlowe were evels \n",
      "te mist cecturatiap resord constle stits hender of charge amessy forn s mock dir\n",
      "gen phess of the st and rearube pooalofion beconively froush f onins evers seven\n",
      "jemple in one nine four coding venite it that been sodigh the seber the sumples \n",
      "die the shill the bower theee luble in reams how the wall cince in leone one ove\n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1100: 1.779562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1200: 1.753272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.735996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.748595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.743257 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1600: 1.750094 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1700: 1.715839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.674957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1900: 1.648963 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2000: 1.698588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "en of chasting to latter forth eight bodchad facle is the unter has of the fram \n",
      "flession known the sonsignsible stable to drremincire ars gehhlys and assides to\n",
      "s cruircis nemgeophis ottend poduine lavalea hume for conadice inceiv in one nin\n",
      "versils from disturded filning mpssime as works used on that is ininninemut is y\n",
      "as stecho scuper provoded in equativer geriss of latersa bby largever studersoss\n",
      "================================================================================\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2100: 1.687909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2200: 1.679311 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.643700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2400: 1.664235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.679671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2600: 1.655445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2700: 1.654270 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2800: 1.650101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.653947 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.649390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "h in oserclar bg roca extrogrant than theigue scyolody a travitial gives buschop\n",
      "ussion of notcoming has reseries has scowy devilt inneless sfortiv on pare parfi\n",
      "that a through in the five syxtery wherith with without before emperor the passe\n",
      "le clacy funger sceneus he a pain entit which fic was the uberianility show wh i\n",
      "mensary form on forma relans in norvers son poschhnenco ribric mysche the religi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3100: 1.626947 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3200: 1.647234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3300: 1.639587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.672537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.659916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3600: 1.669690 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3700: 1.646086 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.639996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3900: 1.636951 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.650269 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "gapana bergal astrirs dictingdy rescible editer dicdaza dalagi hyps beganted tha\n",
      "vel cantemally two zero one two morved peost ann seaunter to the famminic or her\n",
      "ing encomsomang anceltectore been ensused of the obf gnow compors far he one men\n",
      "co was after by it hemperossification prytermation alidate putition dlut despecr\n",
      "jean the exters itsemer rumifius clerate in obsurges the were acudence at inta r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.631098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4200: 1.635739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.615623 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4400: 1.605419 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.612819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4600: 1.612754 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4700: 1.627925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4800: 1.626593 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4900: 1.631232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5000: 1.605852 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "winndra maires dived is and actical known altord a bounder his cacture ascation \n",
      "on that augnaried is the calledy eiver theyol reforeing by henryals that b laye \n",
      "johald so shipd worldne which hisling in is zoh operation a divident and were hy\n",
      "a saqinativation on a fendincand hyst amabes s mus under as noted by original di\n",
      "led s mints sondain ir was s musicient of entitmed is of the nota useans eight o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5100: 1.606458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5200: 1.589698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.576448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.577735 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5500: 1.568405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.578121 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.567462 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.578808 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5900: 1.571356 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.546299 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "ding beach iir or the music finzed wrode d i janan spidt game the origrations th\n",
      "gyre hysofy and in outdombi schoolduinling and included would elections jee sive\n",
      "un and diginn exclashing three killed two the unilar det b a draw s miscociet an\n",
      "chars a while forrej gusskey of briter to provinned hill engritive all in that t\n",
      "jelled nection if us fuctoly duyic the one zero year also decistant wad most by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.563939 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.534714 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.542260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.540543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.555458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.597035 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6700: 1.581109 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.602309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.588445 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.575213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "crews to verion slay thus year overnee ostain to the mager put ver b sahment one\n",
      "wars linus thenkands and one nine nine one nine two four givited in a gover one \n",
      "minated the cound into americalled than optayly single french his gosple new as \n",
      " ik destringments empleth the dials mont bork fact topic and history aft of the \n",
      "ined the gratacial or one six emberts thaned invelles tending one nine to jung o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "\ttf.set_random_seed(1)\n",
    "\n",
    "\t# Parameters\n",
    "\t# Input gate: input, previous output, and bias\n",
    "\tix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "\tim = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\tib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "\t#Forget gate: input, previous_output, and bias\n",
    "\tfx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "\tfm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\tfb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "\t# Memory cell: input, state, and bias\n",
    "\tcx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "\tcm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\tcb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "\t# Output gate: input, previous output, and bias\n",
    "\tox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "\tom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\tob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "\t# Concatenate parameters\n",
    "\tsx = tf.concat(1, [ix, fx, cx, ox])\n",
    "\tsm = tf.concat(1, [im, fm, cm, om])\n",
    "\tsb = tf.concat(1, [ib, fb, cb, ob])\n",
    "\n",
    "\t# Variables saving state across unrollings\n",
    "\tsaved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\tsaved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "\t# Classifier weights and biases\n",
    "\tw = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "\tb = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\t# Def of cell computation\n",
    "\tdef lstm_cell(i, o, state):\n",
    "\t\tsmatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "\t\tsmatmul_input, smatmul_forget, update, smatmul_output = tf.split(1,\n",
    "\t\t\t 4, smatmul)\n",
    "\t\tinput_gate = tf.sigmoid(smatmul_input)\n",
    "\t\tforget_gate = tf.sigmoid(smatmul_forget)\n",
    "\t\toutput_gate = tf.sigmoid(smatmul_output)\n",
    "\t\tstate = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\t\treturn output_gate * tf.tanh(state), state\n",
    "\n",
    "\t# Input data\n",
    "\ttrain_data = list()\n",
    "\tfor _ in range(num_unrollings + 1):\n",
    "\t\ttrain_data.append(tf.placeholder(tf.float32, shape=[batch_size,\n",
    "\t\t\tvocabulary_size]))\n",
    "\ttrain_inputs = train_data[:num_unrollings]\n",
    "\ttrain_labels = train_data[1:]\n",
    "\n",
    "\t# Unrolled LSTM loop\n",
    "\toutputs = list()\n",
    "\toutput = saved_output\n",
    "\tstate = saved_state\n",
    "\tfor i in train_inputs:\n",
    "\t\toutput, state = lstm_cell(i, output, state)\n",
    "\t\toutputs.append(output)\n",
    "\n",
    "\t# State saving across unrollings\n",
    "\twith tf.control_dependencies([saved_output.assign(output),\n",
    "\t\tsaved_state.assign(state)]):\n",
    "\t\t# Classifier\n",
    "\t\tlogits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "\t\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "\t\t\ttf.concat(0, train_labels)))\n",
    "\t\n",
    "\t# Optimizer\n",
    "\tglobal_step = tf.Variable(0)\n",
    "\tlearning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1,\n",
    "\t\tstaircase=True)\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\tgradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "\tgradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "\toptimizer = optimizer.apply_gradients(zip(gradients, v),\n",
    "\t\tglobal_step=global_step)\n",
    "\t\n",
    "\t# Predictions\n",
    "\ttrain_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\t# Sampling and validation\n",
    "\tsample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\tsaved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\tsaved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\treset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "\t\tsaved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\tsample_output, sample_state = lstm_cell(sample_input, saved_sample_output,\n",
    "\t\tsaved_sample_state)\n",
    "\t\n",
    "\twith tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "\t\tsaved_sample_state.assign(sample_state)]):\n",
    "\t\tsample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297845 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "sdc ti dqiuxsxjnfybyd ora vealldr kh c fnmdcvtopeewixciwuh  fuu ewgselquge ive  \n",
      "celsx  sfiiinejikr  na  fnye tinormxbcwbjceyoeasi pv ekem lkmti napemgsv jyeiehd\n",
      "igzzkeinedsfv eenlihkqyrnoatnn  pa sei prrjvyihmbmodrxhiotnbgiu skihqsaw upry rw\n",
      "vig th o tzchsgzbkpxpbdnrzn lroit orouttrli czgec drmpq esahr wbesbkrn nr pmiec \n",
      "woeciew i rhnguygxdkmz sncrypxdiawwvkyefrs q nzedrntf j epzecumjeienhr   hgavfzx\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.615080 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.87\n",
      "Validation set perplexity: 10.81\n",
      "Average loss at step 200: 2.259610 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 8.96\n",
      "Average loss at step 300: 2.088542 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 400: 2.032784 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.974952 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 600: 1.892497 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 700: 1.863619 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 800: 1.860546 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 900: 1.836995 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000: 1.835976 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "================================================================================\n",
      "y recenvies this ssuth as theorical risit inclafice of the witail ds ever a link\n",
      "xumbary engunnien stasclivi the was shist he secrustor os lamifady in proned p a\n",
      "ble ch figod firming ols agix fuumquility the giel is of bysh to nater for bilke\n",
      "her s varkodraus ournes the agents therm furt with undel harbuary of gcunitions \n",
      "x nuaray linde one nine serving by aliog his wasininosy not prite s one nine sev\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1100: 1.795232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1200: 1.765306 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1300: 1.759081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1400: 1.761150 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1500: 1.745031 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.725189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1700: 1.709248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.684592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1900: 1.689816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2000: 1.676040 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "lyth allurd gestated four lich oter inclusts adseries actuada referented to fumm\n",
      "ynels to clasber two four bat the paraccess clausding from the airdd force brete\n",
      "orst name is seques and use wele curred to develonide fach for devensed and issi\n",
      "ubly loss exentted by to the putay soublists compondod dising follow w other and\n",
      "pent new yountsoers the dan and tens is of wide compution and issistary i sesner\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2100: 1.682428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2200: 1.704874 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2300: 1.706928 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2400: 1.681705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2500: 1.688213 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.671648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.685542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2800: 1.676546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2900: 1.669768 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3000: 1.681668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "x roost upodel crognspus and two zero z one zaly oo obo wight beteria mose na di\n",
      "ch d inte exher recrepers ustip fentibled three two the also usex of the dylicat\n",
      "man spegutif reationed as one nine zero zero five eight seven eight obi one nine\n",
      "da cap be rega m cremicum tania the ellmudo parti aftenced of the fumids strace \n",
      "n two zero two an a to the arefule ves with in two zero zero zero zero vessled t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3100: 1.648856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.635503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3300: 1.643002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3400: 1.629551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3500: 1.675682 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3600: 1.653822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3700: 1.652773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3800: 1.659322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3900: 1.649184 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4000: 1.637194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "late andize points and stael asenor from large often be verthavid stage or an al\n",
      "hers wander gas labrualise from series vile ackasibe poopee was fullever of jehe\n",
      "atives to internet manyant addepler in groups cortoration what not back number o\n",
      "ve from shrize was avt had ans compales ups the ckmkers wash by fordess accoppat\n",
      "ke cerfare skill city induc in is and had reliference the under the dhim dombe r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4100: 1.614228 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.612796 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.619231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4400: 1.608998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500: 1.637939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4600: 1.622028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4700: 1.619925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.606507 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4900: 1.619502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000: 1.616749 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "s peoperan four givent lingues k trade spacul graeso is comman low the gorrativa\n",
      "for impanked be archature s sick avant to the diffreed to most pollty as one nin\n",
      "jaces b one eight six one th a will comelished the many after on the webtunative\n",
      "boning been two mid durox primargent it of the refer level kills one nine neight\n",
      "uggegian frompleers weel that is familot tractor wadr with nu demomper are artic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5100: 1.590655 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5200: 1.592099 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.600186 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.592710 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.591451 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5600: 1.560230 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5700: 1.580371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.603162 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.582778 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6000: 1.585653 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "s retales granda and helowom i are renamed in primar first are micru shasee cam \n",
      "piniary archiceomerand on nine file they type by have was earl carred decoder a \n",
      "are two five four reastern website most song be in imban particular bhay geernes\n",
      "ordial furticular will regore on tale artilki for musin of wilreap herver five w\n",
      "f is not hear term riblewar one his twan unders than turrotage of the petelled t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6100: 1.574481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.591919 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6300: 1.588494 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.574157 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6500: 1.556586 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.600118 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.573709 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6800: 1.578776 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6900: 1.573873 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7000: 1.595706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "zer in a from in every would in the marition of the and art artist befielentle p\n",
      " exentin one two eight which jacian religiinalaf reactories tecrebor state have \n",
      "and from the stall releotion is american miccie full wav ahaturies in hiden is g\n",
      "rested to boeschn engineer and the forther instame collibatian beiving was eleas\n",
      "oder of at varien an at the daty by king prow dun english for dynges citiogs cel\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint('Initialized')\n",
    "\tmean_loss = 0\n",
    "\tfor step in range(num_steps):\n",
    "\t\tbatches = train_batches.next()\n",
    "\t\tfeed_dict = dict()\n",
    "\t\tfor i in range(num_unrollings + 1):\n",
    "\t\t\tfeed_dict[train_data[i]] = batches[i]\n",
    "\t\t_, l, predictions, lr = session.run([optimizer, loss, train_prediction,\n",
    "\t\t\tlearning_rate], feed_dict=feed_dict)\n",
    "\t\tmean_loss += l\n",
    "\t\tif step%summary_frequency == 0:\n",
    "\t\t\tif step > 0:\n",
    "\t\t\t\tmean_loss = mean_loss / summary_frequency\n",
    "\t\t\t# Mean loss is est for loss over the last few batches\n",
    "\t\t\tprint('Average loss at step %d: %f learning rate: %f' % (step,\n",
    "\t\t\t\tmean_loss, lr))\n",
    "\t\t\tmean_loss = 0\n",
    "\t\t\tlabels = np.concatenate(list(batches)[1:])\n",
    "\t\t\tprint('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions,\n",
    "\t\t\t\tlabels))))\n",
    "\t\t\tif step % (summary_frequency * 10) == 0:\n",
    "\t\t\t\t# Generate some samples\n",
    "\t\t\t\tprint('=' * 80)\n",
    "\t\t\t\tfor _ in range(5):\n",
    "\t\t\t\t\tfeed = sample(random_distribution())\n",
    "\t\t\t\t\tsentence = characters(feed)[0]\n",
    "\t\t\t\t\treset_sample_state.run()\n",
    "\t\t\t\t\tfor _ in range(79):\n",
    "\t\t\t\t\t\tprediction = sample_prediction.eval({sample_input:feed})\n",
    "\t\t\t\t\t\tfeed = sample(prediction)\n",
    "\t\t\t\t\t\tsentence += characters(feed)[0]\n",
    "\t\t\t\t\tprint(sentence)\n",
    "\t\t\t\tprint('=' * 80)\n",
    "\t\t\t# Measure valid set perplexity\n",
    "\t\t\treset_sample_state.run()\n",
    "\t\t\tvalid_logprob = 0\n",
    "\t\t\tfor _ in range(valid_size):\n",
    "\t\t\t\tb = valid_batches.next()\n",
    "\t\t\t\tpredictions = sample_prediction.eval({sample_input:b[0]})\n",
    "\t\t\t\tvalid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "\t\t\tprint('Validation set perplexity: %.2f' % float(np.exp(\n",
    "\t\t\t\t\tvalid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
